
# https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs#TrainingInput
trainingInput:
  # cheapest
  # scaleTier: BASIC

  # https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs#scaletier
  scaleTier: CUSTOM
  # https://cloud.google.com/compute/docs/machine-types
  # https://cloud.google.com/compute/docs/gpus#a100-gpus
  masterType: n1-standard-4  # 3.75 GB memory per CPU
  # masterType: n1-highmem-4  # 6.5 GB memory per CPU
  # masterType: n1-highcpu-4  # 0.9 GB memory per CPU
  masterConfig:
    imageUri: gcr.io/education-298320/nlp:latest
    acceleratorConfig:
      count: 1
      # https://cloud.google.com/ai-platform/training/docs/using-gpus
      # https://cloud.google.com/compute/docs/gpus#gpu_comparison_chart
      type: NVIDIA_TESLA_K80 # $0.45 per hour per GPU | 12GB memory | 4.4 TFLOPS@FP32 (not enough memory to train roberta large with batch size 1)
      # type: NVIDIA_TESLA_V100 # $2.48 per hour per GPU | 16GB memory | 15.7 TFLOPS@FP32 (not enough memory to train roberta large with batch size 1)
      # type: NVIDIA_A100  # $3.1 per hour per GPU | 40GB memory | 19.5 TFLOPS@FP32 (requires A2 machine type)
      # TPUS are $8 per TPU per hour


      # model is taking 5s/example in training with a batch size of 1